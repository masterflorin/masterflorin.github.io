---
layout: post
title:  "First impressions about the Deep Learning Nanodegree"
date:   2019-02-03 10:54:00 +0200
tags: ['pytorch', 'udacity', 'deep learning', 'nanodegree', 'scholarship']
author: "Florin Cioloboc"
---

*A while ago, I received the [great news](https://twitter.com/florincioloboc/status/1086297229965647872) from Udacity that I've been accepted for the Deep Learning Nanodegree with a scholarship. *

I wanted to write this post as soon as I finished with the first project on Bike sharing, however as soon as I started working on the next lessons left it behind so it's actually going to be shorter than I initially planned it to be. Almost two weeks have passed since I began so it's about time I shared my views.

In summary, in comparison to the [PyTorch Scholarship Challenge](https://masterflorin.github.io/2019/01/11/my-thoughts-pytorch-challenge.html) the nanodegree is considerably longer, twice as long and more challenging as you have more materials and multiple projects to work on which presents itself as a valuable opportunity to build a solid portofolio. I'll start out by covering some of the aspects one at a time.

### Lessons

The Nanodegree (ND) is divided into 6 parts and 7 projects, 5 of which are mandatory for completing the program, supplemented with 2 additional parts of extracurricular material. The first two parts concerning fundamentals of intro to deep learning and PyTorch, as well as some basic material on CNN, RNN can be found in the already existing [Intro to Deep Learning](https://www.udacity.com/course/deep-learning-pytorch--ud188) with the important mention that it is structured differently in the ND. Moreover, the lessons structure make more sense this way thus removing the sense that some material might be missing or that the subject is not covered in sufficient depth. 

From what I covered in these past two weeks two sections are really standing out, the Sentiment Analysis lesson by Andrew Trask and the implementation of gradient descent. Both some common knowedge that you are building your neural network from scratch in Python (using Numpy only) without using any framework which is recommended as the way to go in order to gain intuition into how backpropagation and gradient descent actually works. 

Furthermore, what I found particularly enlightening was Andrew's approach, specifically problem framing. When taking the PyTorch Challenge it did not occur to me in spite of being an elementary topic that one could significantly improve your model's converging speed by applying a few techniques. 

Namely, the goal was improving the capability of generalization by focusing on the predictive theory, increasing the amount of signal and reduced the noise in the dataset, as well as tinkering with the weight initialization. This wasn't a showcase of using something state of the art it outline that tools and techniques of framing a problem are general and apply to any sort of machine learning or deep learning.  

### Community

Much smaller, more compact, not as active as before given that we are only 300 students yet still sufficiently active. At this point the discussions are becoming more focused and are taking place in more specialized groups besides the regular channels.

### Nice things

- We get free GPU (100h) from Udacity so you don't have to worry that your laptop/desktop will buckle under the stress of running deep neural networks, and further AWS credits. 
- The projects we are working on receive feedback from human reviewers.
- 1 Free Github and 1 LinkedIn profile reviews. I also managed to get 1 resume review for free from another promotion which was quite helpful.


All in all, this was my initial impression of the program after almost two weeks, definitely an upgrade from the previous course. I will probably write several posts about the course but with a focus on the projects rather than the whole program itself. 
